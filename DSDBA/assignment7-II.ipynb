{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk-kQYgZ-7FL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import  word_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EurXUDbJ-7Fh"
      },
      "outputs": [],
      "source": [
        "#Importing required module\n",
        "import numpy as np\n",
        "from nltk.tokenize import  word_tokenize \n",
        " \n",
        "#Example text corpus for our tutorial\n",
        "text = ['Topic sentences are similar to mini thesis statements.\\\n",
        "        Like a thesis statement, a topic sentence has a specific \\\n",
        "        main point. Whereas the thesis is the main point of the essay',\\\n",
        "        'the topic sentence is the main point of the paragraph.\\\n",
        "        Like the thesis statement, a topic sentence has a unifying function. \\\n",
        "        But a thesis statement or topic sentence alone doesnâ€™t guarantee unity.', \\\n",
        "        'An essay is unified if all the paragraphs relate to the thesis,\\\n",
        "        whereas a paragraph is unified if all the sentences relate to the topic sentence.']\n",
        " \n",
        "#Preprocessing the text data\n",
        "sentences = []\n",
        "word_set = []\n",
        " \n",
        "for sent in text:\n",
        "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
        "    sentences.append(x)\n",
        "    for word in x:\n",
        "        if word not in word_set:\n",
        "            word_set.append(word)\n",
        "#Set of vocab \n",
        "word_set = set(word_set)\n",
        "#Total documents in our corpus\n",
        "total_documents = len(sentences)\n",
        " \n",
        "#Creating an index for each word in our vocab.\n",
        "index_dict = {} #Dictionary to store index for each word\n",
        "i = 0\n",
        "for word in word_set:\n",
        "    index_dict[word] = i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkZ6rJ_P-7Fr",
        "outputId": "30b3fee6-1ca0-4190-ab83-816d6e0d1b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'has': 2, 'to': 2, 'sentences': 2, 'paragraph': 2, 'an': 1, 'main': 2, 'thesis': 3, 'point': 2, 'unifying': 1, 'are': 1, 'unified': 1, 'unity': 1, 'a': 3, 'topic': 3, 'like': 2, 'all': 1, 'similar': 1, 'function': 1, 'or': 1, 't': 1, 'essay': 2, 'is': 3, 'whereas': 2, 'sentence': 3, 'of': 2, 'statements': 1, 'statement': 2, 'relate': 1, 'guarantee': 1, 'paragraphs': 1, 'mini': 1, 'doesn': 1, 'alone': 1, 'the': 3, 'if': 1, 'but': 1, 'specific': 1}\n"
          ]
        }
      ],
      "source": [
        "#Create a count dictionary\n",
        " \n",
        "def count_dict(sentences):\n",
        "    word_count = {}\n",
        "    for word in word_set:\n",
        "        word_count[word] = 0\n",
        "        for sent in sentences:\n",
        "            if word in sent:\n",
        "                word_count[word] += 1\n",
        "    return word_count\n",
        " \n",
        "word_count = count_dict(sentences)\n",
        "print(word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCEgaflP-7Fy"
      },
      "outputs": [],
      "source": [
        "#Term Frequency\n",
        "def termfreq(document, word):\n",
        "    N = len(document)\n",
        "    \n",
        "    occurance = len([token for token in document if token == word])\n",
        "    return occurance/N\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s93yGXK-7F1",
        "outputId": "16434307-1f3f-4856-e8ed-5ce1597ef691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "#  for testing \n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCbdpWrk-7F4"
      },
      "outputs": [],
      "source": [
        "#Inverse Document Frequency\n",
        " \n",
        "def inverse_doc_freq(word):\n",
        "    try:\n",
        "        word_occurance = word_count[word] + 1\n",
        "    except:\n",
        "        word_occurance = 1\n",
        "    return np.log(total_documents/word_occurance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7E6F4cR-7F7"
      },
      "outputs": [],
      "source": [
        "def tf_idf(sentence):\n",
        "    tf_idf_vec = np.zeros((len(word_set),))\n",
        "    for word in sentence:\n",
        "        tf = termfreq(sentence,word)\n",
        "        idf = inverse_doc_freq(word)\n",
        "         \n",
        "        value = tf*idf\n",
        "        tf_idf_vec[index_dict[word]] = value \n",
        "    return tf_idf_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUAIe9UN-7F_"
      },
      "outputs": [],
      "source": [
        "#TF-IDF Encoded text corpus\n",
        "vectors = []\n",
        "for sent in sentences:\n",
        "    vec = tf_idf(sent)\n",
        "    vectors.append(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09etob6r-7GC",
        "outputId": "28483bb8-327a-4cdf-ca14-80f5fc3bab98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.02876821  0.          0.          0.          0.          0.0135155\n",
            "  0.          0.          0.0135155  -0.0095894  -0.02876821  0.\n",
            "  0.          0.0135155   0.          0.         -0.0191788   0.0135155\n",
            "  0.          0.          0.          0.          0.0135155   0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            " -0.02876821  0.          0.          0.          0.          0.\n",
            " -0.0095894 ]\n"
          ]
        }
      ],
      "source": [
        "print(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPgiKXoQ-7GJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "DSBDA-assignment-7-II.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}