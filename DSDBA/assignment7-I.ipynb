{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', ',', 'just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "['I will walk 500 miles and I would walk 500 more, just to be the man who walks a thousand miles to fall down at your door!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = \"I will walk 500 miles and I would walk 500 more, just to be the man who walks a thousand miles to fall down at your door!\"\n",
    "print(word_tokenize(sent))\n",
    "print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', ',', 'just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "This is the cleaned version: ['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', ',', 'man', 'walks', 'thousand', 'miles', 'fall', 'door', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords        # the corpus module is an \n",
    "                                         # extremely useful one. \n",
    "                                         # More on that later.\n",
    "stop_words = stopwords.words('english')  # this is the full list of\n",
    "                                         # all stop-words stored in\n",
    "                                         # nltk\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version:\", token)\n",
    "print(\"This is the cleaned version:\", cleaned_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'play', 'playful', 'play']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "This is when ‘fluff’ letters (not words) are removed from a word and grouped together with its “stem form”. \n",
    "For instance, the words ‘play’, ‘playing’, or ‘plays’ convey the same meaning (although, again, not exactly, \n",
    "but for analysis with a computer, that sort of detail is still not a viable option). \n",
    "So instead of having them as different words, we can put them together under the same umbrella term ‘play’.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = ['play', 'playing', 'plays', 'played',\n",
    "         'playfullness', 'playful']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We used the PorterStemmer, which is a pre-written stemmer class. \n",
    "There are other stemmers like SnowballStemmer and LancasterStemmer but PorterStemmer is sort of the simplest one.\n",
    "‘Play’ and ‘Playful’ should have been recognized as two different words however. \n",
    "Notice how the last ‘playful’ got recognized as ‘play’ and not ‘playful’. \n",
    "This is where the simplicity of the PorterStemmer is undesirable. \n",
    "You can also train your own using unsupervised clustering or supervised classification ML models. \n",
    "Now let’s stem an actual sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I play the play play as the player were play in the play with playful \n"
     ]
    }
   ],
   "source": [
    "sent2 = \"I played the play playfully as the players were playing in the play with playfullness\"\n",
    "token = word_tokenize(sent2)\n",
    "stemmed = \"\"\n",
    "for word in token:\n",
    "    stemmed += stemmer.stem(word) + \" \"\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('walk', 'VBP'), ('500', 'CD'), ('miles', 'NNS'), ('I', 'PRP'), ('would', 'MD'), ('walk', 'VB'), ('500', 'CD'), (',', ','), ('man', 'NN'), ('walks', 'NNS'), ('thousand', 'VBP'), ('miles', 'NNS'), ('fall', 'VB'), ('door', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Tagging Parts of Speech (pos)The next essential thing we want to do is tagging each word in the corpus \n",
    "#(a corpus is just a ‘bag’ of words) we created after converting sentences by tokenizing.\n",
    "\n",
    "from nltk import pos_tag \n",
    "token = word_tokenize(sent) + word_tokenize(sent2)\n",
    "tagged = pos_tag(cleaned_token)                 \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
